对比维度,TrioSim,SimAI,Lumos,Proteus,vTrain,ASTRA-sim 2.0,Galvatron,Vidur,LLMServingSim,TokenSim,AMALI,LLMShare,APEX,SamuLLM
类别,训练,训练,训练,训练,训练,训练,训练,推理,推理,推理,推理,推理,推理,推理
机构/团队,SArchLab（多校联合）,Alibaba Cloud（阿里云）,工业界/学术界合作团队,研究团队,研究团队,多机构合作（包括Northwestern等）,Peking University DAIR（北大河图团队）,Microsoft,KAIST,Peking University Lemonade,研究团队,研究团队,研究团队,研究团队
论文来源,ISCA 2025,USENIX NSDI 2025,arXiv 2025（预印本）,arXiv 2023,arXiv 2023,arXiv 2023（论文/框架）,VLDB 2023,详见论文,详见论文,详见论文,ISCA 2025,DAC 2025,2024-2025,arXiv（详见论文）
开源仓库,论文提供实现信息；开源状态以论文主页为准,论文与演讲材料公开；代码/工具可用性以官方发布为准,论文公开；代码开源状态以作者主页为准,研究原型为主；实际工程需对接训练框架,研究原型；开源/可用性以作者发布为准,开源生态成熟（ASTRA-sim项目）,论文与部分代码/资料公开（以官方为准）,详见论文主页,详见论文主页,详见论文主页,详见论文主页,详见论文主页,详见论文主页,详见论文主页
输入内容,训练框架profiler/通信库统计/硬件与网络参数,profiling trace、通信库与网络测量、硬件参数,Kineto trace、硬件参数（带宽/显存等）,模型图/层统计+硬件/通信参数（必要时小量profiling）,profiling trace、硬件与网络参数、成本模型,模型抽象描述+平台配置参数（可选用外部测量）,模型图/层统计、硬件与通信参数、（可选）短profiling,算子画像（离线测量）；真实请求轨迹；必要的系统参数（批上限、并发、KV缓存策略）,来自PyTorch/TensorRT-LLM等的性能CSV、少量实测标定；系统参数（并行/批/内存）,真实数据集采样的到达与长度分布；必要的系统参数与少量测量,GPU参数（峰值算力/带宽/并发特性）与模型算子统计；少量实测校准点,系统配置、LLM信息、请求轨迹；必要的成本模型参数,模型图/层统计、硬件参数（带宽/显存）、必要的测量特征,"大型数据集预先统计的输出长度经验累积分布函数(eCDF)；请求集合(workload)信息：请求输入长度、请求类别(如generation/summarization/rewrite)、模型数、各模型的依赖关系(computation graph)等"
输出指标,训练时间、吞吐、利用率、资源效率,训练时间、吞吐、利用率,训练迭代时间、吞吐、利用率,训练吞吐、迭代时间,训练时间、成本、利用率,训练时间、通信瓶颈、内存占用,并行策略、训练时间估计、显存占用,TTFT、TPOT/ITL、E2E延迟分布、吞吐、GPU利用率、成本（可派生）,TTFT、TPOT、系统端到端延迟、吞吐、利用率,延迟分布、吞吐、利用率、KV命中率/内存占用等,阶段时延、瓶颈定位（计算/带宽Roofline视角）,服务时间分布、成本、利用率、SLA达成率,计划代价、端到端时间估计、显存占用,"端到端推理总延迟/总时间（end-to-end latency/total runtime，报告速度提升1.0-2.4×）；硬件资源利用效率（GPU空闲时间少、吞吐率提升）；成本节省"
建模过程描述,分布式训练仿真器：基于trace的大规模训练时间与资源效率评估。详细过程：收集训练trace（含算子或阶段时序、通信事件）→构建多GPU/节点执行与通信模型→重放并估算训练时长与利用率,统一式训练系统仿真平台：计算、通信、网络一体化高保真模拟。详细过程：在目标/代表性环境采集计算与通信画像→构建执行与网络模型→重放训练负载并支持多配置对比,基于PyTorch Kineto trace的训练迭代任务级重放与配置评估。详细过程：用Kineto采集迭代内任务依赖图与时间片→在改变并行/配置时重放估算新的迭代时长,并行策略树表示+执行图仿真评估训练吞吐。详细过程：将并行方案编码为策略树→编译为执行图→模拟计算与通信重叠、带宽竞争，估算吞吐,Profiling驱动的训练系统配置与并行策略模拟与成本评估。详细过程：以实测获得子系统性能画像→模拟不同配置/策略的训练时间与成本,分布式训练平台级模拟（计算-通信-内存-网络联动）。详细过程：以抽象任务图+网络/内存模型模拟大规模训练平台行为，定位通信/内存瓶颈,自动混合并行训练系统，内置代价模型与策略搜索。详细过程：构建代价模型（计算/通信/内存）→决策树/DP搜索混合并行（DP/SDP/TP/PP）组合→选择最优并行策略,对模型各算子做离线profiling得到性能画像；收集真实请求轨迹（到达过程、prompt/生成长度）；以trace驱动在仿真器中重放，建模批处理、并发、调度与资源竞争，输出端到端指标,以层/阶段CSV与profile为输入，建模prefill与decode的时间演化；支持计算复用以加速大规模仿真；可与ASTRA-Sim/Chakra联动评估网络/互联瓶颈,将动态请求分布作为一等输入；提供可插拔模块评估调度、批策略、KV缓存/内存管理等；快速A/B对比策略,基于GPU微架构参数与LLM算子特征推导闭式或半闭式模型；少量标定后进行快速what-if/敏感度分析,构建含成本与服务时间的目标函数；对服务器规模、资源切分与扩缩容策略进行搜索与模拟评估，形成DSE闭环,对不同并行组合建立代价/资源模型，估计计算与通信代价，自动搜索满足显存与性能权衡的计划,采用采样-然后-仿真(sampling-then-simulation)模型：先从经验累积分布函数(eCDF)抽样估计输出长度→用仿真模拟推理过程、计算每轮迭代延迟→基于此估计模型执行时间
建模粒度/层级,层/任务（流水线）/设备/系统/网络,kernel/算子/任务/设备/系统/网络,任务/设备/系统（可映射并行维度）,层/任务/设备/系统（并行计划级）,任务/设备/系统,系统/网络（支持映射模型并行策略）,层/设备/系统（并行计划级）,算子→阶段（prefill/decode）→工作负载/系统,迭代/阶段→模型/设备→系统集群,阶段/策略→系统服务,算子/阶段→设备级性能估计,服务配置/路由→多服务器系统,并行计划/算子划分→设备/集群,任务/模型/系统（多LLM应用级）
硬件测量需求,是（为高精度需在代表性硬件上采集trace）；亦可辅以抽象模型,是（需要硬件实测进行画像/参数标定）,是（需要在目标/相似硬件上采集Kineto trace）,可选（支持用硬件参数与通信带宽建模；实测用于校准更佳）,是（依赖profiling数据进行画像与校准）,否（可用理论/配置参数建模；也可用实测校准更准确）,可选（代价模型可用理论+少量测量校准更准）,依赖：需要在目标或代表性GPU上进行算子级profiling（如PyTorch/Triton/TensorRT、CUDA events），获得算子时延/吞吐-随shape/批大小曲线；同时收集真实请求轨迹。跨硬件可复用流程，但需重新画像以保证精度,弱~中度依赖：可直接复用现有CSV（源自PyTorch profiler或TensorRT-LLM）；若需更高精度，建议在目标硬件上跑短基准导出层/阶段时延表；网络/互联参数可来自ASTRA-Sim/Chakra或实测带宽/时延,低~中度依赖：核心是负载分布与策略模块；为获得更准确绝对值，建议对prefill/decode基元做轻量profiling（tokens/s、KV读写带宽）或从线上监控导入统计,低依赖：以解析模型为主，使用公开GPU参数（峰值算力、带宽、SM数等）与模型算子统计；仅需少量校准点（微基准/短跑）来拟合常数项或利用率上限,取决于上游：框架本身不强制硬件测量，但需要性能/成本参数。可引用Vidur/LLMServingSim/AMALI等的结果，或进行短时实测以构建prefill/decode成本/时延曲线,中度依赖：需要模型图与硬件通信/计算参数；可通过短基准收集NCCL带宽/时延、kernel效率、显存峰值等，亦可复用厂商/集群既有参数库,是（需要通过大规模数据集预先运行模型以获取输出长度的经验累积分布函数eCDF）
测量粒度与校准方式,算子/阶段时延、通信（如AllReduce）时间或带宽、网络拓扑参数等,kernel/算子时间，集体通信性能（如NCCL），网络带宽/时延、拓扑等,任务/算子时间线（Kineto）、通信事件、设备利用率等,通信带宽/时延、设备峰值算力/显存、模型层统计等,计算/通信基元吞吐、设备利用率、可能的成本参数,网络拓扑/带宽/时延、内存层次、并行策略描述,单机/多机通信带宽、层计算时间（可通过短profiling获取）、显存约束等,算子画像（离线测量）；真实请求轨迹,来自PyTorch/TensorRT-LLM等的性能CSV、少量实测标定,真实数据集采样的到达与长度分布；必要的系统参数与少量测量,GPU参数（峰值算力/带宽/并发特性）与模型算子统计；少量实测校准点,系统配置、LLM信息、请求轨迹；必要的成本模型参数,模型图/层统计、硬件参数（带宽/显存）、必要的测量特征,"校准：通过大规模数据集预先运行模型获取输出长度eCDF；验证：使用多个应用场景(3个应用+1混合应用)在真实/模拟的多模型推理任务上与基线调度算法比较"
额外特性,多GPU、多并行维度（数据/张量/流水）、网络拓扑可配置,支持多模型与大规模GPU集群，网络拓扑可切换,多模型（GPT-3 15B/44B/117B/175B等）、最多512×H100,支持多种并行维度组合与多硬件配置,多租户GPU集群、并行策略组合,支持多拓扑、分层内存、并行策略联动；可与上层工具耦合,支持多并行维度组合、适配多GPU,多模型（LLaMA/OPT等）；GPU集群；不同批策略、并行与资源配置；可比较不同集群/实例规格；适用于容量/成本规划。模块化可插拔，适配多模型与多机集群；可纳入容量规划与SLA评估流程,支持GPU与NPU；多设备（100+）扩展；多种并行与服务策略；便于HW/SW共设计。模块化极强，可插硬件与编译器栈；面向联合设计与大规模评估,多模型/多配置；强调策略空间实验（调度、内存、批）；适配GPU集群。策略层模块化，易与研究代码集成进行快速验证,面向主流GPU；适合不同模型/算子组合的快速评估（需提供参数）。无需大规模仿真即可做方案扫描；可作为上游筛选器,多服务器、多配置；prefill/decode前后端资源分摊；可结合多模型SLA/预算。可与容量规划、成本控制流程集成；策略搜索可拓展,GPU集群；支持多种并行维度的组合；可扩展到不同模型图。可嵌入自动并行/编译器流水线作为决策器,"专注多LLM应用的离线批量推理；自下而上调度和资源分配；将任务抽象为计算图；支持并行运行；允许模型preemption(中断、重新调度)以提高资源利用"
寻优算法/机制,并行策略与系统配置的搜索/比较（以仿真评估代价）,以仿真为内环的配置/架构参数探索（支持快速方案对比）,并行/配置更改后的迭代时间预测，用于快速方案筛选,在大策略空间中比较/筛选并行计划,以训练时间/成本为目标的配置搜索/比较,平台/网络维度的设计空间探索与瓶颈分析,动态规划等在大空间中寻优最优并行策略,端到端trace驱动仿真与系统配置评估,基于层/阶段profile的时间演化建模与系统级仿真,策略层可插拔模块与快速A/B对比,解析建模与快速参数扫描,设计空间探索与成本优化（DSE）,并行计划代价模型与自动搜索,"采样-仿真方法：不依赖精确预测每个请求输出长度，基于长度分布估计运行时间；贪婪搜索算法：每阶段选择能提升每GPU吞吐率最多的模型及执行计划；动态运行时调整计划（若实际与估计偏差）"
精度/效果,论文报告与实测对齐良好（具体误差请以论文表格/图示为准）,论文报告仿真结果与实测训练高度一致（≈98%一致性；以原文数据为准）,论文报告在生产集群上平均误差约3.3%（以原文为准）,论文报告平均预测误差约3%（以原文为准）,论文给出多案例评估结果（具体数值以原文为准）,作为平台模拟器，论文展示案例分析，无统一误差指标,论文在多模型上优于若干基线（具体提升幅度以原文为准）,对真实GPU系统吞吐/延迟误差<~5%（典型工作负载）,对真实GPU系统误差<~14.7%；较传统加速器仿真提速≈×91,部分基准下误差<~1%，能稳定区分策略优劣,报告在若干GPU/模型上预测误差较小（数量级级别）,在其DSE流程内有效收敛至更优成本-性能点（相对基线）,能较好预测不同并行策略的优劣并给出可执行方案,实验结果显示相对基线速度提升1.0-2.4×；在多个应用场景上验证有效性
缺陷/限制,部分底层协议/微结构抽象；对输入trace质量较敏感,准备成本较高；网络/通信画像需随环境变化维护,暂未覆盖能耗/成本等；默认假设配置可成功执行,偏重策略层评估；系统/存储细节抽象,细节粒度相对系统级；对画像质量较敏感,不直接建模kernel级细节；需与上游画像/框架结合,更偏"自动并行"系统，非全栈仿真；对代价模型准确性敏感,对高质量画像与轨迹依赖大；画像更新与维护成本较高,算子级细节不如算子仿真；需要准备性能CSV/外部模拟器集成,对底层互联/网络细节抽象较强；通用性依赖负载复现实度,不覆盖队列/调度/网络；需与系统仿真结合验证,底层算子/互联不建模；精度依赖上游性能/成本模型质量,不含服务层队列与到达过程；需与系统级仿真/实测结合,依赖预先统计的eCDF质量；主要面向离线批量推理场景；详见论文
